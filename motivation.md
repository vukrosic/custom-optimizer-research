Making any progress in neural network optimizers will have massive ripple effects on the field of AI research and development.

We don't even need to come up with a new optimizer, actually, the best way to come up with a new optimizer is to understand the current ones, and why they work.

To come up with new optimizers, we shoudl focus on understanding why current ones work, as opposed to coming up with new ideas. Ideas will be a lot better once we understand why current ones work..

In this work we will do extensive experiments to gain practical and theoretical understanding of optimizers, especially why the new Muon optimizer outperforms other optimizers.

We will use 2 experiments: MNIST and LLM. Each will have different or same optimizers or optimizer components applied and tested.

1. Muon
2. Manifold Muon
3. 