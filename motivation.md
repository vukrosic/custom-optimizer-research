Making any progress in neural network optimizers will have massive ripple effects on the field of AI research and development.

We don't even need to come up with a new optimizer, actually, the best way to come up with a new optimizer is to understand the current ones, and why they work.

In this work we will do extensive experiments to gain practical and theoretical understanding of optimizers, especially why the new Muon optimizer outperforms other optimizers.

We will use 3 experiments: MNIST, CIFAR10 and LLM. Each will have different or same optimizers or optimizer components applied and tested.

1. Muon
2. Manifold Muon
3. 